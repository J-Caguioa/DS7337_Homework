{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joseph Caguioa\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "DS 7337: Natural Language Processing\n",
    "\n",
    "Section 404 (Tuesday 2030-2200)\n",
    "\n",
    "HW6 Due: Date of Live Session 11 (3/17/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><a name=\"toc\">Table of Contents:</a></u>\n",
    "* [Question 1](#question1)\n",
    "* [Question 2](#question2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question1\">Question 1</a> \n",
    "\n",
    "<b>Evaluate text similarity of Amazon book search results by doing the following: \n",
    "  \n",
    "* Do a book search on Amazon. Manually copy the full book title (including subtitle) of each of the top 24 books listed in the first two pages of search results.\n",
    "* In Python, run one of the text-similarity measures covered in this course, e.g., cosine similarity. Compare each of the book titles, pairwise, to every other one.\n",
    "* Which two titles are the most similar to each other? Which are the most dissimilar? Where do they rank, among the first 24 results?</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During these days of quarantine, the coronavirus COVID-19 is on everyone's mind. Thus, my Amazon book search topic is on something entirely different for a fun juxtaposition: the sun. In other words, the star at the center of our solar system that many people will not see for themselves until early April, and which happens to have an outer atmosphere called a corona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://spaceplace.nasa.gov/sun-corona/en/corona1.en.jpg\" width=\"150\" height=\"150\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://spaceplace.nasa.gov/sun-corona/en/corona1.en.jpg\", \n",
    "      width=150, height=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format is first specified as paperback; even with the advent of audiobooks and Kindle-enabled virtual books, physical copies rule. Among the top 24 results, left as sorted by \"Featured,\" are several marked as bestsellers. Results noted as sponsored ads, duplicates, or entries that are not single books (e.g., a trilogy of several books packaged together) are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['The Sun Is Also a Star',\n",
    "          'Embrace the Sun',\n",
    "          'The Sun and Her Flowers',\n",
    "          'The Sun Does Shine: How I Found Life, Freedom, and Justice',\n",
    "          'A Thousand Splendid Suns',\n",
    "          'A Raisin in the Sun',\n",
    "          'The Sun Chaser',\n",
    "          'How Dare the Sun Rise: Memoirs of a War Child',\n",
    "          'The Warmth of Other Suns: The Epic Story of America\\'s Great Migration',\n",
    "          'I\\'ll Give You the Sun',\n",
    "          'The Sun Also Rises: The Hemingway Library Edition',\n",
    "          'Dear Sunshine,: A Children\\'s Story About The Positive Impact Of The Sun',\n",
    "          'The Weight of Stars and Suns',\n",
    "          'Tortilla Sun',\n",
    "          'The Sun: Our Nearest Star (Let\\'s-Read-and-Find-Out)',\n",
    "          'Where The Sun Goes',\n",
    "          'Besos de sol, abrazos de luna Sun Kisses, Moon Hugs (Spanish Edition) (Spanish and English Edition)',\n",
    "          'Gene Wolfe\\'s The Book of the New Sun: A Chapter Guide',\n",
    "          'Half of a Yellow Sun',\n",
    "          'When the Sun Bursts: The Enigma of Schizophrenia',\n",
    "          'The Medicine Wheel: Earth Astrology',\n",
    "          'Twin Suns Of Carrola (Starshatter)',\n",
    "          'The Sun Is a Compass: My 4,000-Mile Journey into the Alaskan Wilds',\n",
    "          'In the Old Sun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code modified from https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "title_matrix = count_vectorizer.fit_transform(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.51639778 0.4        0.28284271 0.         0.4472136\n",
      "  0.51639778 0.2981424  0.2236068  0.4        0.56568542 0.3721042\n",
      "  0.18257419 0.31622777 0.42426407 0.4472136  0.09534626 0.38729833\n",
      "  0.2236068  0.42426407 0.2        0.         0.47809144 0.4472136 ]\n",
      " [0.51639778 1.         0.51639778 0.36514837 0.         0.57735027\n",
      "  0.66666667 0.38490018 0.28867513 0.51639778 0.54772256 0.48038446\n",
      "  0.23570226 0.40824829 0.36514837 0.57735027 0.12309149 0.5\n",
      "  0.28867513 0.54772256 0.25819889 0.         0.46291005 0.57735027]\n",
      " [0.4        0.51639778 1.         0.42426407 0.         0.4472136\n",
      "  0.51639778 0.2981424  0.2236068  0.4        0.42426407 0.3721042\n",
      "  0.36514837 0.31622777 0.42426407 0.4472136  0.19069252 0.38729833\n",
      "  0.2236068  0.42426407 0.2        0.         0.35856858 0.4472136 ]\n",
      " [0.28284271 0.36514837 0.42426407 1.         0.         0.31622777\n",
      "  0.36514837 0.31622777 0.15811388 0.28284271 0.3        0.26311741\n",
      "  0.25819889 0.2236068  0.3        0.31622777 0.13483997 0.27386128\n",
      "  0.15811388 0.3        0.14142136 0.         0.25354628 0.31622777]\n",
      " [0.         0.         0.         0.         1.         0.\n",
      "  0.         0.         0.14433757 0.         0.         0.\n",
      "  0.23570226 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.25819889 0.         0.        ]\n",
      " [0.4472136  0.57735027 0.4472136  0.31622777 0.         1.\n",
      "  0.57735027 0.33333333 0.25       0.4472136  0.47434165 0.41602515\n",
      "  0.20412415 0.35355339 0.31622777 0.5        0.10660036 0.4330127\n",
      "  0.25       0.47434165 0.2236068  0.         0.40089186 0.75      ]\n",
      " [0.51639778 0.66666667 0.51639778 0.36514837 0.         0.57735027\n",
      "  1.         0.38490018 0.28867513 0.51639778 0.54772256 0.48038446\n",
      "  0.23570226 0.40824829 0.36514837 0.57735027 0.12309149 0.5\n",
      "  0.28867513 0.54772256 0.25819889 0.         0.46291005 0.57735027]\n",
      " [0.2981424  0.38490018 0.2981424  0.31622777 0.         0.33333333\n",
      "  0.38490018 1.         0.33333333 0.2981424  0.31622777 0.36980013\n",
      "  0.27216553 0.23570226 0.21081851 0.33333333 0.07106691 0.38490018\n",
      "  0.33333333 0.42163702 0.1490712  0.1490712  0.26726124 0.33333333]\n",
      " [0.2236068  0.28867513 0.2236068  0.15811388 0.14433757 0.25\n",
      "  0.28867513 0.33333333 1.         0.2236068  0.31622777 0.48536267\n",
      "  0.51031036 0.         0.15811388 0.25       0.         0.4330127\n",
      "  0.25       0.47434165 0.2236068  0.3354102  0.26726124 0.25      ]\n",
      " [0.4        0.51639778 0.4        0.28284271 0.         0.4472136\n",
      "  0.51639778 0.2981424  0.2236068  1.         0.42426407 0.3721042\n",
      "  0.18257419 0.31622777 0.28284271 0.4472136  0.09534626 0.38729833\n",
      "  0.2236068  0.42426407 0.2        0.         0.35856858 0.4472136 ]\n",
      " [0.56568542 0.54772256 0.42426407 0.3        0.         0.47434165\n",
      "  0.54772256 0.31622777 0.31622777 0.42426407 1.         0.43852901\n",
      "  0.25819889 0.2236068  0.3        0.47434165 0.20225996 0.45643546\n",
      "  0.15811388 0.5        0.28284271 0.         0.42257713 0.47434165]\n",
      " [0.3721042  0.48038446 0.3721042  0.26311741 0.         0.41602515\n",
      "  0.48038446 0.36980013 0.48536267 0.3721042  0.43852901 1.\n",
      "  0.33968311 0.19611614 0.26311741 0.41602515 0.05913124 0.48038446\n",
      "  0.2773501  0.52623481 0.24806947 0.12403473 0.37062466 0.41602515]\n",
      " [0.18257419 0.23570226 0.36514837 0.25819889 0.23570226 0.20412415\n",
      "  0.23570226 0.27216553 0.51031036 0.18257419 0.25819889 0.33968311\n",
      "  1.         0.         0.25819889 0.20412415 0.08703883 0.35355339\n",
      "  0.20412415 0.38729833 0.18257419 0.36514837 0.21821789 0.20412415]\n",
      " [0.31622777 0.40824829 0.31622777 0.2236068  0.         0.35355339\n",
      "  0.40824829 0.23570226 0.         0.31622777 0.2236068  0.19611614\n",
      "  0.         1.         0.2236068  0.35355339 0.15075567 0.20412415\n",
      "  0.35355339 0.2236068  0.         0.         0.18898224 0.35355339]\n",
      " [0.42426407 0.36514837 0.42426407 0.3        0.         0.31622777\n",
      "  0.36514837 0.21081851 0.15811388 0.28284271 0.3        0.26311741\n",
      "  0.25819889 0.2236068  1.         0.31622777 0.13483997 0.27386128\n",
      "  0.15811388 0.3        0.14142136 0.         0.25354628 0.31622777]\n",
      " [0.4472136  0.57735027 0.4472136  0.31622777 0.         0.5\n",
      "  0.57735027 0.33333333 0.25       0.4472136  0.47434165 0.41602515\n",
      "  0.20412415 0.35355339 0.31622777 1.         0.10660036 0.4330127\n",
      "  0.25       0.47434165 0.2236068  0.         0.40089186 0.5       ]\n",
      " [0.09534626 0.12309149 0.19069252 0.13483997 0.         0.10660036\n",
      "  0.12309149 0.07106691 0.         0.09534626 0.20225996 0.05913124\n",
      "  0.08703883 0.15075567 0.13483997 0.10660036 1.         0.06154575\n",
      "  0.10660036 0.06741999 0.         0.         0.05698029 0.10660036]\n",
      " [0.38729833 0.5        0.38729833 0.27386128 0.         0.4330127\n",
      "  0.5        0.38490018 0.4330127  0.38729833 0.45643546 0.48038446\n",
      "  0.35355339 0.20412415 0.27386128 0.4330127  0.06154575 1.\n",
      "  0.28867513 0.54772256 0.25819889 0.12909944 0.38575837 0.4330127 ]\n",
      " [0.2236068  0.28867513 0.2236068  0.15811388 0.         0.25\n",
      "  0.28867513 0.33333333 0.25       0.2236068  0.15811388 0.2773501\n",
      "  0.20412415 0.35355339 0.15811388 0.25       0.10660036 0.28867513\n",
      "  1.         0.31622777 0.         0.2236068  0.13363062 0.25      ]\n",
      " [0.42426407 0.54772256 0.42426407 0.3        0.         0.47434165\n",
      "  0.54772256 0.42163702 0.47434165 0.42426407 0.5        0.52623481\n",
      "  0.38729833 0.2236068  0.3        0.47434165 0.06741999 0.54772256\n",
      "  0.31622777 1.         0.28284271 0.14142136 0.42257713 0.47434165]\n",
      " [0.2        0.25819889 0.2        0.14142136 0.         0.2236068\n",
      "  0.25819889 0.1490712  0.2236068  0.2        0.28284271 0.24806947\n",
      "  0.18257419 0.         0.14142136 0.2236068  0.         0.25819889\n",
      "  0.         0.28284271 1.         0.         0.23904572 0.2236068 ]\n",
      " [0.         0.         0.         0.         0.25819889 0.\n",
      "  0.         0.1490712  0.3354102  0.         0.         0.12403473\n",
      "  0.36514837 0.         0.         0.         0.         0.12909944\n",
      "  0.2236068  0.14142136 0.         1.         0.         0.        ]\n",
      " [0.47809144 0.46291005 0.35856858 0.25354628 0.         0.40089186\n",
      "  0.46291005 0.26726124 0.26726124 0.35856858 0.42257713 0.37062466\n",
      "  0.21821789 0.18898224 0.25354628 0.40089186 0.05698029 0.38575837\n",
      "  0.13363062 0.42257713 0.23904572 0.         1.         0.40089186]\n",
      " [0.4472136  0.57735027 0.4472136  0.31622777 0.         0.75\n",
      "  0.57735027 0.33333333 0.25       0.4472136  0.47434165 0.41602515\n",
      "  0.20412415 0.35355339 0.31622777 0.5        0.10660036 0.4330127\n",
      "  0.25       0.47434165 0.2236068  0.         0.40089186 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "title_similarities = cosine_similarity(title_matrix, title_matrix)\n",
    "print(title_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to find most similar and most dissimilar\n",
    "# np.where(title_similarities == title_similarities[title_similarities > 0.0].min())[0]\n",
    "#title_similarities[title_similarities > 0.0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most similar titles are <em>A Raisin in the Sun</em> and <em>In the Old Sun</em>, the 4th and 24th results to appear; the cosine similarity between them is 0.75. This is due to the exclusion of stopwords. Without those, the two titles share \"Sun\" in common and only differ by \"Raisin\" and \"Old.\" Something similar happens for <em>Embrace the Sun</em> and <em>The Sun Chaser</em>, which would only be off by one word if stopwords were included. In general, the shortest titles (or those pared down with stopwords removed) seem to have the highest similarities with each other.\n",
    "\n",
    "<u>Top Three Similarities</u>\n",
    "1. <em>A Raisin in the Sun</em> (4) & <em>In the Old Sun</em> (24): 0.750\n",
    "2. <em>Embrace the Sun</em> (2) & <em>The Sun Chaser</em> (7): 0.667\n",
    "3. Different combinations between ranks 2, 7, 12, 16, 24: 0.577\n",
    "\n",
    "The most dissimilar titles are combinations with no words in common, resulting in a cosine similarity of 0.0. While  nearly all titles that appeared contain \"sun,\" there is no differentiation between the singular and plural forms in my implementation. The two next most dissimilar combinations occur with longer titles with few non-stopwords in common, with one title containing Spanish. In these instances, having more unshared content words drives down the cosine similarity. \n",
    "\n",
    "<u>Top Three Dissimilarities</u>\n",
    "1. Various combinations of books not sharing content words: 0.0\n",
    "2. <em>Besos de sol, abrazos de luna Sun Kisses, Moon Hugs (Spanish Edition) (Spanish and English Edition)</em> (17) & <em>The Sun Is a Compass: My 4,000-Mile Journey into the Alaskan Wilds</em> (23): 0.0570\n",
    "3. <em>Dear Sunshine,: A Children's Story About The Positive Impact Of The Sun</em> (12) & <em>Besos de sol, abrazos de luna Sun Kisses, Moon Hugs (Spanish Edition) (Spanish and English Edition)</em> (17): 0.0591\n",
    "\n",
    "Overall, there does not appear to be an obvious relationship between cosine similarity and ranking in the results (e.g., higher similarities being between higher ranked results). Instead, cosine similarity appears more to be a function of the number of non-stopwords compared across titles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question2\">Question 2</a> \n",
    "\n",
    "<b>Now evaluate using a major search engine.\n",
    "\n",
    "* Enter one of the book titles from question 1a into Google, Bing, or Yahoo!. Copy the capsule of the first organic result and the 20th organic result. Take web results only (i.e., not video results), and skip sponsored results.  \n",
    "* Run the same text similarity calculation that you used for question 1b on each of these capsules in comparison to the original query (book title).\n",
    "* Which one has the highest similarity measure?</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glance at the summary indicates its contents likely have little to do with tortillas as a main idea, but <em>Tortilla Sun</em> is chosen because of its fun title and the wordplay possibilities from results that did not make the front page. (Additionally, it is one of the few titles that is not also the name of a movie.) \n",
    "\n",
    "Bing's first organic capsule is a relevant review of the book on the social cataloging website Goodreads. In contrast, the twentieth organic capsule is a not-as-relevant but tasty-sounding recipe for tortilla soup from Food Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title14 = \"Tortilla Sun\"\n",
    "capsule1 = \"Tortilla Sun had me longing to see the Sandia Mountains, feel the warmth of the sun and hear the call of the wind. Bright New Mexico comes as vividly alive as the colors worn by many of its people. This book is recommended for ages 9 to 12, but I think girls up to 14 or 15 may enjoy it too.\"\n",
    "capsule20 = \"Simmer Trisha Yearwood's Chicken Tortilla Soup recipe, from Trisha's Southern Kitchen on Food Network, with a jar of salsa, fajita seasoning and canned veggies. All Tortilla Soup Recipes Ideas.\"\n",
    "capsule_compare = [title14, capsule1, capsule20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.20701967 0.24253563]\n",
      " [0.20701967 1.         0.10041929]\n",
      " [0.24253563 0.10041929 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "capsule_matrix = count_vectorizer.fit_transform(capsule_compare)\n",
    "capsule_similarities = cosine_similarity(capsule_matrix, capsule_matrix)\n",
    "print(capsule_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough, the tortilla soup recipe has a slightly higher cosine similarity (0.243) than the book review (0.207) to the book title. This is simply because the review has more non-stopwords than the recipe, and that weighs it down when comparing with the two-word book title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.43989914 0.17977937]\n",
      " [0.43989914 1.         0.10041929]\n",
      " [0.17977937 0.10041929 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "title14_desc = \"When twelve-year-old Izzy spends the summer in her Nana's remote New Mexico village, she discovers long-buried secrets that come alive in an enchanted landscape of majestic mountains, whispering winds, and tortilla suns. Infused with the flavor of the southwest and sprinkled with just a pinch of magic, readers are sure to find this heartfelt story as rich and satisfying as Nana's homemade enchiladas.\"\n",
    "capsule_compare2 = [title14_desc, capsule1, capsule20]\n",
    "capsule_matrix2 = count_vectorizer.fit_transform(capsule_compare2)\n",
    "capsule_similarities2 = cosine_similarity(capsule_matrix2, capsule_matrix2)\n",
    "print(capsule_similarities2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purposes, Amazon's description of <em>Tortilla Sun</em> is compared against the capsules. Considering that the plot summary contains descriptions of the setting and plot, some of which are mirrored by the reviewer, it is expected that the book review capsule has a higher cosine similarity this time (0.440).\n",
    "\n",
    "This exercise supports the idea that when it comes to search result display rankings, both on retail sites like Amazon and major engines like Bing, more than just the text similarities are considered when determining what is most relevant to display."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
