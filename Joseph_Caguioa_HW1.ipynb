{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joseph Caguioa\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "DS 7337: Natural Language Processing\n",
    "\n",
    "Section 404 (Tuesday 2030-2200)\n",
    "\n",
    "HW1 Due: Date of Live Session 2 (1/14/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><a name=\"toc\">Table of Contents:</a></u>\n",
    "* [Question 1](#question1)\n",
    "* [Question 2](#question2)\n",
    "* [Question 3](#question3)\n",
    "* [Question 4](#question4)\n",
    "* [Question 5](#question5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question1\">Question 1</a> \n",
    "\n",
    "<b>Install Python (if you don’t have it already), and install NLTK.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.4\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'my', 'first', 'homework', 'assignment', '.', 'Natural', 'language', 'processing', 'looks', 'interesting', '!', 'This', 'block', 'demonstrates', 'that', 'the', 'nltk', 'package', 'was', 'successfully', 'installed', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "example_sent = \"This is my first homework assignment.\\\n",
    "    Natural language processing looks interesting! \\\n",
    "    This block demonstrates that the nltk package was successfully installed.\"\n",
    "print(word_tokenize(example_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is presented in a Jupyter Notebook through Anaconda. The above code chunks show that Python and NLTK have been installed on my machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question2\">Question 2</a>\n",
    "\n",
    "<b>Q2: Follow the instructions in chapter 1 of Bird-Klein for implementing a “lexical diversity” scoring routine.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the textbook's definition, lexical diversity is the ratio of unique words to total words in a text. The closer to 1.0, the more unique the entirety of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition from pg9 of Bird/Klein/Loper's NLP with Python\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the function has been correctly defined by using known texts and verifying that the expected values are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n",
      "The lexical diversity of text3 is 0.06230453042623537.\n",
      "The lexical diversity of text5 is 0.13477005109975562.\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text3, text5\n",
    "print(f\"The lexical diversity of text3 is {lexical_diversity(text3)}.\")\n",
    "print(f\"The lexical diversity of text5 is {lexical_diversity(text5)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question3\">Question 3</a>\n",
    "\n",
    "<b>Q3: Go to http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf), and obtain three texts (of different grade levels) from the “Graded Readers” section. Report the lexical diversity score of each. Explain whether the result was surprising.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For consistency in comparing \"similar\" texts, Graded Readers from the same series are chosen. Here, plain texts of McGuffey's [Ordinal] Eclectic Reader are imported. No editing, such as removal of the copyright notice, transcriber's notes, or illustration notes, is done, as it is assumed their contents are relatively similar across texts and will not significantly impact the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package needed to access texts as urls\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_url(url):\n",
    "    \"\"\"\n",
    "    Function to obtain text from a web address and return list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    url (string): Url of interest\n",
    "    \n",
    "    Returns:\n",
    "    list: Text from url as list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    response = http.request('GET', url)\n",
    "    raw = response.data.decode('utf-8')\n",
    "    return word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_url = 'http://www.gutenberg.org/cache/epub/14880/pg14880.txt'\n",
    "fourth_tokens = tokenize_url(fourth_url)\n",
    "\n",
    "fifth_url = 'http://www.gutenberg.org/cache/epub/15040/pg15040.txt'\n",
    "fifth_tokens = tokenize_url(fifth_url)\n",
    "\n",
    "sixth_url = 'http://www.gutenberg.org/cache/epub/16751/pg16751.txt'\n",
    "sixth_tokens = tokenize_url(sixth_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick look at the first few tokens verifies that the correct texts were successfully read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of McGuffey 's Fourth Eclectic Reader by William Holmes McGuffey\n",
      "﻿The Project Gutenberg EBook of McGuffey 's Fifth Eclectic Reader by William Holmes McGuffey\n",
      "﻿The Project Gutenberg EBook of McGuffey 's Sixth Eclectic Reader by William Holmes McGuffey\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(fourth_tokens[:14]))\n",
    "print(' '.join(fifth_tokens[:14]))\n",
    "print(' '.join(sixth_tokens[:14]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the lexical diversity function created in Q2 is applied to these three texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lexical diversity of McGuffey's Fourth Eclectic Reader is 0.12354.\n",
      "The lexical diversity of Mcguffey's Fifth Eclectic Reader is 0.11289.\n",
      "The lexical diversity of McGuffey's Sixth Eclectic Reader is 0.10088.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The lexical diversity of McGuffey's Fourth Eclectic Reader is \\\n",
    "{round(lexical_diversity(fourth_tokens),5)}.\")\n",
    "print(f\"The lexical diversity of Mcguffey's Fifth Eclectic Reader is \\\n",
    "{round(lexical_diversity(fifth_tokens),5)}.\")\n",
    "print(f\"The lexical diversity of McGuffey's Sixth Eclectic Reader is \\\n",
    "{round(lexical_diversity(sixth_tokens),5)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance this is a surprising trend&mdash;later books in the sequence appear to have <em>lower</em> lexical diversity. Shouldn't it be the reverse, with textbooks that teach literacy and grammar increasing in difficulty and introducing more varied vocabulary at higher levels? This indeed occurs, and a look at the denominator of the lexical diversity formula, the length of the text (in terms of tokens), can help explain it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of McGuffey's Fourth Eclectic Reader is 84044.\n",
      "The length of McGuffey's Fifth Eclectic Reader is 126604.\n",
      "The length of McGuffey's Sixth Eclectic Reader is 171089.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The length of McGuffey's Fourth Eclectic Reader is {len(fourth_tokens)}.\")\n",
    "print(f\"The length of McGuffey's Fifth Eclectic Reader is {len(fifth_tokens)}.\")\n",
    "print(f\"The length of McGuffey's Sixth Eclectic Reader is {len(sixth_tokens)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown here, successive textbooks are increasingly longer; the Sixth Eclectic Reader is more than twice the size of the Fourth Eclectic Reader! Sheer length likely outpaces word uniqueness. No matter how complex the vocabulary, sentences will still need repetitive pieces like articles, conjunctions, and punctuation. Acknowledging the token length helps reconcile the apparent anomaly with the decreasing lexical diversity scores.\n",
    "\n",
    "Additionally, a brief study into the style used in these textbooks reveals that the methodology involves introduction and frequent reuse of new words. By design, this would drive down lexical diversity by diluting variability in favor of intentional repetition to help students learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question4\">Question 4</a>\n",
    "\n",
    "<b>Q4: Also compare the vocabulary size of the same three texts. Explain whether the result was surprising.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the textbook, vocabulary size can be understood as the number of unique words, ignoring capitalization and non-alphabetic tokens. Again, the texts are considered without removing the copyright notice, transcriber's notes, or illustration notes, as these are assumed negligible in the overall comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition from pg25 of Bird/Klein/Loper's NLP with Python\n",
    "def vocab_size(text):\n",
    "    return len(set(word.lower() for word in text if word.isalpha()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7629 unique words in McGuffey's Fourth Eclectic Reader.\n",
      "There are 10836 unique words in McGuffey's Fifth Eclectic Reader.\n",
      "There are 13713 unique words in McGuffey's Sixth Eclectic Reader.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {vocab_size(fourth_tokens)} unique words in McGuffey's Fourth Eclectic Reader.\")\n",
    "print(f\"There are {vocab_size(fifth_tokens)} unique words in McGuffey's Fifth Eclectic Reader.\")\n",
    "print(f\"There are {vocab_size(sixth_tokens)} unique words in McGuffey's Sixth Eclectic Reader.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more straightforward result that likely aligns with expectations. Successive books in the series are expected to build upon their predecessors, and that includes vocabulary. Hence, vocabulary size should increase with difficulty level, and this trend is indeed observed. However, it is also possible that this is a function of the readers getting longer, and thus introducing more opportunities for novel words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question5\">Question 5</a>\n",
    "\n",
    "<b>Q5: Write a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By themselves, either value has the potential to be misleading. As shown in Question 3, the objectively hardest text of the three has the lowest lexical diversity despite having the largest vocabulary. Putting both together and understanding the context&mdash;that text length drives down lexical diversity despite more opportunities for unique vocabulary&mdash;gives a more complete picture. Since vocabulary size is a subset of token length under the given function definitions, it can be used as a proxy for calculating a low estimate for total text length (demonstrated in the formulas below). That text length then helps better frame both values: \"There are {Vocabulary Size} unique words in a text bigger than at least {Calculated Length}, resulting in a lexical diversity value of {Lexical Diversity}.\" This knowledge helps standardize any comparisons, especially when the texts are of different sizes, and gives a better baseline to approximate the text's difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "LexicalDiversity = \\frac{TokenLength}{TextLength} \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "VocabLength \\subset TokenLength\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\therefore TextLength \\geq \\hat{TextLength} = \\frac{VocabLength}{LexicalDiversity}\n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
