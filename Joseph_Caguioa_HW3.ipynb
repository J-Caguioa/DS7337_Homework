{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joseph Caguioa\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "DS 7337: Natural Language Processing\n",
    "\n",
    "Section 404 (Tuesday 2030-2200)\n",
    "\n",
    "HW3 Due: Date of Live Session 6 (2/11/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><a name=\"toc\">Table of Contents:</a></u>\n",
    "* [Question 1](#question1)\n",
    "* [Question 2](#question2)\n",
    "* [Question 3](#question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question1\">Question 1</a> \n",
    "\n",
    "<b>Compare your given name with your nickname (if you don’t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "  \n",
    "* What is the edit distance between your nickname and your given name?\n",
    "* What is the percentage string match between your nickname and your given name?\n",
    "\n",
    "Show your work for both calculations.\n",
    "</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My given name, Joseph, has a natural nickname, Joe. From a suffix perspective, a person's first instinct might be to remove the -seph and then tack on the -e, a total of edits involving five characters through deletion and insertion. However, on a by-character basis, it's actually a simple deletion of just three: 's', 'p', and 'h.' Hence, the minimum edit distance is 3. \n",
    "\n",
    "Another natural nickname is Joey. For this nickname the minimum edit distance is actually still 3, even though a new character is introduced. This time, the edits involve deletion of two characters and substitution of one. \n",
    "\n",
    "Both transformations are demonstrated below. In the bottom row for each diagram, 'd' indicates deletion and 's' indicates substitution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| | 0 | 1 | 2 | 3 | 4 | 5 |\n",
    "| - | - | - | - | - | - | - |\n",
    "| Given Name: | J | O | S | E | P | H |\n",
    "| Nickname: | J | O | * | E | * | * |\n",
    "| Edits: 3  | - | - | d | - | d | d |\n",
    "\n",
    "<em><b>Figure 1</b></em>: Joseph to Joe\n",
    "\n",
    "| | 0 | 1 | 2 | 3 | 4 | 5 |\n",
    "| - | - | - | - | - | - | - |\n",
    "| Given Name: | J | O | S | E | P | H |\n",
    "| Nickname: | J | O | * | E | Y | * |\n",
    "| Edits: 3  | - | - | d | - | s | d |\n",
    "\n",
    "<em><b>Figure 2</b></em>: Joseph to Joey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Levenshtein package contains several helpful functions for string analysis. One calculates Levenshtein edit distance—a measurement of the number of character-based additions, deletions, and substitutions to transform one string to another—which was shown above. While a substitution could be treated as a consecutive deletion and insertion, two separate operations, it is treated as just one by the edit distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein as leven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The edit distance between 'Joseph' and 'Joseph' is 0.\n",
      "The edit distance between 'Joseph' and 'Joe' is 3.\n",
      "The edit distance between 'Joseph' and 'Joey' is 3.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The edit distance between 'Joseph' and 'Joseph' is {leven.distance('Joseph', 'Joseph')}.\") # Expects 0\n",
    "print(f\"The edit distance between 'Joseph' and 'Joe' is {leven.distance('Joseph', 'Joe')}.\") # Expects 3\n",
    "print(f\"The edit distance between 'Joseph' and 'Joey' is {leven.distance('Joseph', 'Joey')}.\") # Expects 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage string match could be described as another similarity measurement that compares string similarity using the edit distance, but also directly accounts for potential length disparities.\n",
    "\n",
    "\\begin{equation*}\n",
    "percent\\_match = \\frac{len(s) + len(t) - distance}{len(s) + len(t)} * 100\\%\n",
    "\\end{equation*}\n",
    "\n",
    "The edit distance values calculated above can be plugged into this equation. Note, however, that the Levenshtein package's implementation for the ratio() function, which returns percentage string match, does penalize substitutions. That is factored into the math as well. So, for the Joseph-Joey comparison, the substitution is effectively treated as a deletion and insertion, and edit distance is 4 instead of 3.\n",
    "\n",
    "\\begin{equation*}\n",
    "percent\\_match( \\text{'Joseph', 'Joe'} ) = \\frac{6 + 3 - 3}{6 + 3} * 100 = \\frac{6}{9} * 100 = 67\\%\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "percent\\_match( \\text{'Joseph', 'Joey'} ) = \\frac{6 + 4 - 4}{6 + 4} * 100 = \\frac{6}{10} * 100 = 60\\%\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent string match between 'Joseph' and 'Joseph' is 100.0%.\n",
      "The percent string match between 'Joseph' and 'Joe' is 66.7%.\n",
      "The percent string match between 'Joseph' and 'Joey' is 60.0%.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The percent string match between 'Joseph' and 'Joseph' is {leven.ratio('Joseph', 'Joseph')*100}%.\")\n",
    "print(f\"The percent string match between 'Joseph' and 'Joe' is {(round(leven.ratio('Joseph', 'Joe'), 3)*100)}%.\")\n",
    "print(f\"The percent string match between 'Joseph' and 'Joey' is {leven.ratio('Joseph', 'Joey')*100}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joey, which has one unique character not found in the original Joseph, has a lower percentage match compared with Joe. This makes intuitive sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question2\">Question 2</a> \n",
    "\n",
    "<b>Find a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think your friend either was or was not able to guess the book from hearing the list of words.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some imports are called and functions created to grab a text of interest available online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# Tokenizer function from HW1\n",
    "def tokenize_url(url):\n",
    "    \"\"\"\n",
    "    Function to obtain text from a web address and return list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        url (string): Url of interest\n",
    "    \n",
    "    Returns:\n",
    "        list: Text from url as list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    response = http.request('GET', url)\n",
    "    raw = response.data.decode('utf-8')\n",
    "    return word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab A Tale of Two Cities from Gutenberg and tokenize it.\n",
    "tale_url = 'http://www.gutenberg.org/files/98/98-0.txt'\n",
    "tale_tokens = tokenize_url(tale_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way--in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only . There were a king with a large jaw and a queen with a plain face, on the throne of England; there were a king with a large jaw and a queen with a fair face, on the throne of France.\n"
     ]
    }
   ],
   "source": [
    "# Find indices for the slice returning first 2 sentences.\n",
    "tale_tokens.index('It') # Returns 385\n",
    "tale_tokens.index('France') # Returns 565\n",
    "two_sentences = TreebankWordDetokenizer().detokenize(tale_tokens[385:567])\n",
    "print(two_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the first two rather long tokenized sentences of Charles Dickens' famous 1859 historical fiction novel <em>A Tale of Two Cities</em>. At a glance, many of these are stop words that will get removed, including articles, conjunctions, prepositions, and conjugations of \"to be.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Function to remove stopwords. Borrowed from Bird/Klein/Loper pg. 120.\n",
    "    Modified to also convert all tokens to lowercase and remove tokens with non-alphabetic characters.\n",
    "    \n",
    "    Parameters:\n",
    "        tokens (list): Tokenized list\n",
    "        \n",
    "    Returns:\n",
    "        filtered_tokens (list): Tokenized list without stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    stopword_list = stopwords.words('english')\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'best times worst times age wisdom age foolishness epoch belief epoch incredulity season light season darkness spring hope winter despair everything us nothing us going direct heaven going direct way short period far like present period noisiest authorities insisted received good evil superlative degree comparison king large jaw queen plain face throne england king large jaw queen fair face throne france'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words from first 2 sentences.\n",
    "two_sentences_tokens = tale_tokens[385:567]\n",
    "two_sentences_tokens_filtered = remove_stopwords(two_sentences_tokens)\n",
    "TreebankWordDetokenizer().detokenize(two_sentences_tokens_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output shows the remaining content words after passing through the filter for common stop words. From 182 tokens in the original two sentences of <em>A Tale of Two Cities</em>, only 61, or 33%, remain. Yet even with this list, my friend was able to guess the title after some prodding...although his first was \"Some annoying book I was forced to read back in grade school English.\" Although the stop words were stripped away, the \"best times, worst times\" opening is pretty recognizable. At the very least, most people with some exposure to English literature in their lifetime would likely feel they have heard that cadence and sequence before, even if the actual book title escaped them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'authorities', 'belief', 'best', 'comparison', 'darkness', 'degree', 'despair', 'direct', 'england', 'epoch', 'everything', 'evil', 'face', 'fair', 'far', 'foolishness', 'france', 'going', 'good', 'heaven', 'hope', 'incredulity', 'insisted', 'jaw', 'king', 'large', 'light', 'like', 'noisiest', 'nothing', 'period', 'plain', 'present', 'queen', 'received', 'season', 'short', 'spring', 'superlative', 'throne', 'times', 'us', 'way', 'winter', 'wisdom', 'worst']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens_set = sorted(set(two_sentences_tokens_filtered))\n",
    "print(filtered_tokens_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had the sentences been reduced and sorted further into an actual list, specifically the filtered unique 47-token set, it is unlikely my friend would have successfully identified the book. The repetition of \"times\" in those opening words makes a big difference. However, given the set shown above, certain key words could give potential hints. England, France, and the presence of nobility (\"king,\" \"queen,\" \"throne\") likely place the setting somewhere in Europe, especially either of the named countries. London, England and Paris, France are, in fact, the eponymous two cities. A word like \"epoch,\" uncommonly used in contemporary vernacular, may age the writer as someone from a different generation, or perhaps someone attempting to paint a scene in an older time, perhaps back when France also still had a monarchy. One would likely be inclined to think of older literary works, prone to open with long exposition—47 <em>unique</em> tokens in the first two sentences feels quite high. Still, guessing with the set on its own could be a tall order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question3\">Question 3</a> \n",
    "\n",
    "<b>Run one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird/Klein/Loper points out two common stemmers that use different sets of rules to remove affixes. For curiosity's sake, both are implemented below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "ps = PorterStemmer()\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ps_stemmer(tokens):\n",
    "    stemmed = [ps.stem(token) for token in tokens]\n",
    "    return stemmed\n",
    "\n",
    "def ls_stemmer(tokens):\n",
    "    stemmed = [ls.stem(token) for token in tokens]\n",
    "    return stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It wa the best of time, it wa the worst of time, it wa the age of wisdom, it wa the age of foolish, it wa the epoch of belief, it wa the epoch of incredul, it wa the season of light, it wa the season of dark, it wa the spring of hope, it wa the winter of despair, we had everyth befor us, we had noth befor us, we were all go direct to heaven, we were all go direct the other way--in short, the period wa so far like the present period, that some of it noisiest author insist on it be receiv, for good or for evil, in the superl degre of comparison onli . there were a king with a larg jaw and a queen with a plain face, on the throne of england; there were a king with a larg jaw and a queen with a fair face, on the throne of franc.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sentences_ps = ps_stemmer(two_sentences_tokens)\n",
    "TreebankWordDetokenizer().detokenize(two_sentences_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it was the best of tim, it was the worst of tim, it was the ag of wisdom, it was the ag of fool, it was the epoch of believ, it was the epoch of incred, it was the season of light, it was the season of dark, it was the spring of hop, it was the wint of despair, we had everyth bef us, we had noth bef us, we wer al going direct to heav, we wer al going direct the oth way--in short, the period was so far lik the pres period, that som of it noisiest auth insist on it being receiv, for good or for evil, in the superl degr of comparison on . ther wer a king with a larg jaw and a queen with a plain fac, on the throne of england; ther wer a king with a larg jaw and a queen with a fair fac, on the throne of frant.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_sentences_ls = ls_stemmer(two_sentences_tokens)\n",
    "TreebankWordDetokenizer().detokenize(two_sentences_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each stemmer handled the sentences a little differently. The Porter stemmer, for example, stripped \"was\" to \"wa.\" The Lancaster stemmer frequently removed ending -e in instances such as \"like\" to \"lik,\" \"some\" to \"som,\" and \"there\" to \"ther.\" Both removed -ing, changing \"everything\" to \"everyth\" and \"nothing\" to \"noth.\"\n",
    "\n",
    "Comparing the stems may be easier if the punctuation is removed and the tokens are used only once. Thus, the filtered token set is also run through the stemmers below. For comparison, the results of the two stemmers are also shown alongside the WordNet lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Token</th>\n",
       "      <th>Porter Stems</th>\n",
       "      <th>Lancaster Stems</th>\n",
       "      <th>Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>age</td>\n",
       "      <td>age</td>\n",
       "      <td>ag</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>authorities</td>\n",
       "      <td>author</td>\n",
       "      <td>auth</td>\n",
       "      <td>authority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>belief</td>\n",
       "      <td>belief</td>\n",
       "      <td>believ</td>\n",
       "      <td>belief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>comparison</td>\n",
       "      <td>comparison</td>\n",
       "      <td>comparison</td>\n",
       "      <td>comparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>darkness</td>\n",
       "      <td>dark</td>\n",
       "      <td>dark</td>\n",
       "      <td>darkness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>degree</td>\n",
       "      <td>degre</td>\n",
       "      <td>degr</td>\n",
       "      <td>degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>despair</td>\n",
       "      <td>despair</td>\n",
       "      <td>despair</td>\n",
       "      <td>despair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>england</td>\n",
       "      <td>england</td>\n",
       "      <td>england</td>\n",
       "      <td>england</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>epoch</td>\n",
       "      <td>epoch</td>\n",
       "      <td>epoch</td>\n",
       "      <td>epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>everything</td>\n",
       "      <td>everyth</td>\n",
       "      <td>everyth</td>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>face</td>\n",
       "      <td>face</td>\n",
       "      <td>fac</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>far</td>\n",
       "      <td>far</td>\n",
       "      <td>far</td>\n",
       "      <td>far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>foolishness</td>\n",
       "      <td>foolish</td>\n",
       "      <td>fool</td>\n",
       "      <td>foolishness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>france</td>\n",
       "      <td>franc</td>\n",
       "      <td>frant</td>\n",
       "      <td>france</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>going</td>\n",
       "      <td>go</td>\n",
       "      <td>going</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>heaven</td>\n",
       "      <td>heaven</td>\n",
       "      <td>heav</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>hope</td>\n",
       "      <td>hope</td>\n",
       "      <td>hop</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>incredulity</td>\n",
       "      <td>incredul</td>\n",
       "      <td>incred</td>\n",
       "      <td>incredulity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>insisted</td>\n",
       "      <td>insist</td>\n",
       "      <td>insist</td>\n",
       "      <td>insisted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>jaw</td>\n",
       "      <td>jaw</td>\n",
       "      <td>jaw</td>\n",
       "      <td>jaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>king</td>\n",
       "      <td>king</td>\n",
       "      <td>king</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>large</td>\n",
       "      <td>larg</td>\n",
       "      <td>larg</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>lik</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>noisiest</td>\n",
       "      <td>noisiest</td>\n",
       "      <td>noisiest</td>\n",
       "      <td>noisiest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>nothing</td>\n",
       "      <td>noth</td>\n",
       "      <td>noth</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>period</td>\n",
       "      <td>period</td>\n",
       "      <td>period</td>\n",
       "      <td>period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>pres</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>queen</td>\n",
       "      <td>queen</td>\n",
       "      <td>queen</td>\n",
       "      <td>queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>received</td>\n",
       "      <td>receiv</td>\n",
       "      <td>receiv</td>\n",
       "      <td>received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>season</td>\n",
       "      <td>season</td>\n",
       "      <td>season</td>\n",
       "      <td>season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>superlative</td>\n",
       "      <td>superl</td>\n",
       "      <td>superl</td>\n",
       "      <td>superlative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>throne</td>\n",
       "      <td>throne</td>\n",
       "      <td>throne</td>\n",
       "      <td>throne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>times</td>\n",
       "      <td>time</td>\n",
       "      <td>tim</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>winter</td>\n",
       "      <td>winter</td>\n",
       "      <td>wint</td>\n",
       "      <td>winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Token Porter Stems Lancaster Stems       Lemmas\n",
       "0             age          age              ag          age\n",
       "1     authorities       author            auth    authority\n",
       "2          belief       belief          believ       belief\n",
       "3            best         best            best         best\n",
       "4      comparison   comparison      comparison   comparison\n",
       "5        darkness         dark            dark     darkness\n",
       "6          degree        degre            degr       degree\n",
       "7         despair      despair         despair      despair\n",
       "8          direct       direct          direct       direct\n",
       "9         england      england         england      england\n",
       "10          epoch        epoch           epoch        epoch\n",
       "11     everything      everyth         everyth   everything\n",
       "12           evil         evil            evil         evil\n",
       "13           face         face             fac         face\n",
       "14           fair         fair            fair         fair\n",
       "15            far          far             far          far\n",
       "16    foolishness      foolish            fool  foolishness\n",
       "17         france        franc           frant       france\n",
       "18          going           go           going        going\n",
       "19           good         good            good         good\n",
       "20         heaven       heaven            heav       heaven\n",
       "21           hope         hope             hop         hope\n",
       "22    incredulity     incredul          incred  incredulity\n",
       "23       insisted       insist          insist     insisted\n",
       "24            jaw          jaw             jaw          jaw\n",
       "25           king         king            king         king\n",
       "26          large         larg            larg        large\n",
       "27          light        light           light        light\n",
       "28           like         like             lik         like\n",
       "29       noisiest     noisiest        noisiest     noisiest\n",
       "30        nothing         noth            noth      nothing\n",
       "31         period       period          period       period\n",
       "32          plain        plain           plain        plain\n",
       "33        present      present            pres      present\n",
       "34          queen        queen           queen        queen\n",
       "35       received       receiv          receiv     received\n",
       "36         season       season          season       season\n",
       "37          short        short           short        short\n",
       "38         spring       spring          spring       spring\n",
       "39    superlative       superl          superl  superlative\n",
       "40         throne       throne          throne       throne\n",
       "41          times         time             tim         time\n",
       "42             us           us              us            u\n",
       "43            way          way             way          way\n",
       "44         winter       winter            wint       winter\n",
       "45         wisdom       wisdom          wisdom       wisdom\n",
       "46          worst        worst           worst        worst"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk import WordNetLemmatizer\n",
    "tokens_ps = ps_stemmer(filtered_tokens_set)\n",
    "tokens_ls = ls_stemmer(filtered_tokens_set)\n",
    "wnl = WordNetLemmatizer()\n",
    "tokens_lemmas = [wnl.lemmatize(t) for t in filtered_tokens_set]\n",
    "compare = zip(filtered_tokens_set, tokens_ps, tokens_ls, tokens_lemmas)\n",
    "pd.DataFrame(compare, columns=[\"Original Token\", \"Porter Stems\", \"Lancaster Stems\", \"Lemmas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no apparent programmatic way to check the validity of the stems as morphological roots of the original words, so this was done manually. Note that instances where the stemmer removed affixes but still produced valid stems are considered valid in this context if the stems have related meanings. So for example, \"foolishness\" to \"foolish\" and \"fool\" are valid, but \"hope\" to \"hop\" is not.\n",
    "\n",
    "* Porter valid roots (37): age, belief, best, comparison, dark, despair, direct, england, epoch, evil, face, fair, far, foolish, go, good, heaven, hope, insist, jaw, king, light, like, period, plain, present, queen, season, short, spring, throne, time, us, way, winter, wisdom, worst\n",
    "* Lancaster valid roots (27): best, comparison, dark, despair, direct, england, epoch, evil, fair, far, fool, good, insist, jaw, king, light, period, plain, queen, season, short, spring, throne, us, way, wisdom, worst\n",
    "\n",
    "When done on the filtered set of 46 tokens, the Porter stemmer achieved 80.43% valid roots while the Lancaster stemmer achieved 58.69%. Reintroducing the stop words, as shown below, returns similar results, with 83.09% and 60.56%, respectively. Though, one could argue the slight uptick could be an artificial increase considering that many stop words (like \"a,\" \"and,\" \"of\") are already in root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Token</th>\n",
       "      <th>Porter Stems</th>\n",
       "      <th>Lancaster Stems</th>\n",
       "      <th>Lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>age</td>\n",
       "      <td>age</td>\n",
       "      <td>ag</td>\n",
       "      <td>age</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>al</td>\n",
       "      <td>all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>authorities</td>\n",
       "      <td>author</td>\n",
       "      <td>auth</td>\n",
       "      <td>authority</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>before</td>\n",
       "      <td>befor</td>\n",
       "      <td>bef</td>\n",
       "      <td>before</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>being</td>\n",
       "      <td>be</td>\n",
       "      <td>being</td>\n",
       "      <td>being</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>belief</td>\n",
       "      <td>belief</td>\n",
       "      <td>believ</td>\n",
       "      <td>belief</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>comparison</td>\n",
       "      <td>comparison</td>\n",
       "      <td>comparison</td>\n",
       "      <td>comparison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>darkness</td>\n",
       "      <td>dark</td>\n",
       "      <td>dark</td>\n",
       "      <td>darkness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>degree</td>\n",
       "      <td>degre</td>\n",
       "      <td>degr</td>\n",
       "      <td>degree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>despair</td>\n",
       "      <td>despair</td>\n",
       "      <td>despair</td>\n",
       "      <td>despair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>england</td>\n",
       "      <td>england</td>\n",
       "      <td>england</td>\n",
       "      <td>england</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>epoch</td>\n",
       "      <td>epoch</td>\n",
       "      <td>epoch</td>\n",
       "      <td>epoch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>everything</td>\n",
       "      <td>everyth</td>\n",
       "      <td>everyth</td>\n",
       "      <td>everything</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "      <td>evil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>face</td>\n",
       "      <td>face</td>\n",
       "      <td>fac</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "      <td>fair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>far</td>\n",
       "      <td>far</td>\n",
       "      <td>far</td>\n",
       "      <td>far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>foolishness</td>\n",
       "      <td>foolish</td>\n",
       "      <td>fool</td>\n",
       "      <td>foolishness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>france</td>\n",
       "      <td>franc</td>\n",
       "      <td>frant</td>\n",
       "      <td>france</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>going</td>\n",
       "      <td>go</td>\n",
       "      <td>going</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>heaven</td>\n",
       "      <td>heaven</td>\n",
       "      <td>heav</td>\n",
       "      <td>heaven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>hope</td>\n",
       "      <td>hope</td>\n",
       "      <td>hop</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>incredulity</td>\n",
       "      <td>incredul</td>\n",
       "      <td>incred</td>\n",
       "      <td>incredulity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>insisted</td>\n",
       "      <td>insist</td>\n",
       "      <td>insist</td>\n",
       "      <td>insisted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>its</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>jaw</td>\n",
       "      <td>jaw</td>\n",
       "      <td>jaw</td>\n",
       "      <td>jaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>king</td>\n",
       "      <td>king</td>\n",
       "      <td>king</td>\n",
       "      <td>king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>large</td>\n",
       "      <td>larg</td>\n",
       "      <td>larg</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>like</td>\n",
       "      <td>like</td>\n",
       "      <td>lik</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>noisiest</td>\n",
       "      <td>noisiest</td>\n",
       "      <td>noisiest</td>\n",
       "      <td>noisiest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>nothing</td>\n",
       "      <td>noth</td>\n",
       "      <td>noth</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>only</td>\n",
       "      <td>onli</td>\n",
       "      <td>on</td>\n",
       "      <td>only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "      <td>or</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>oth</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>period</td>\n",
       "      <td>period</td>\n",
       "      <td>period</td>\n",
       "      <td>period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "      <td>plain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>present</td>\n",
       "      <td>present</td>\n",
       "      <td>pres</td>\n",
       "      <td>present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>queen</td>\n",
       "      <td>queen</td>\n",
       "      <td>queen</td>\n",
       "      <td>queen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>received</td>\n",
       "      <td>receiv</td>\n",
       "      <td>receiv</td>\n",
       "      <td>received</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>season</td>\n",
       "      <td>season</td>\n",
       "      <td>season</td>\n",
       "      <td>season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "      <td>so</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>some</td>\n",
       "      <td>some</td>\n",
       "      <td>som</td>\n",
       "      <td>some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "      <td>spring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>superlative</td>\n",
       "      <td>superl</td>\n",
       "      <td>superl</td>\n",
       "      <td>superlative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>there</td>\n",
       "      <td>there</td>\n",
       "      <td>ther</td>\n",
       "      <td>there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>throne</td>\n",
       "      <td>throne</td>\n",
       "      <td>throne</td>\n",
       "      <td>throne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>times</td>\n",
       "      <td>time</td>\n",
       "      <td>tim</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>us</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>was</td>\n",
       "      <td>wa</td>\n",
       "      <td>was</td>\n",
       "      <td>wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>were</td>\n",
       "      <td>were</td>\n",
       "      <td>wer</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>winter</td>\n",
       "      <td>winter</td>\n",
       "      <td>wint</td>\n",
       "      <td>winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>wisdom</td>\n",
       "      <td>wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Original Token Porter Stems Lancaster Stems       Lemmas\n",
       "0               a            a               a            a\n",
       "1             age          age              ag          age\n",
       "2             all          all              al          all\n",
       "3             and          and             and          and\n",
       "4     authorities       author            auth    authority\n",
       "5          before        befor             bef       before\n",
       "6           being           be           being        being\n",
       "7          belief       belief          believ       belief\n",
       "8            best         best            best         best\n",
       "9      comparison   comparison      comparison   comparison\n",
       "10       darkness         dark            dark     darkness\n",
       "11         degree        degre            degr       degree\n",
       "12        despair      despair         despair      despair\n",
       "13         direct       direct          direct       direct\n",
       "14        england      england         england      england\n",
       "15          epoch        epoch           epoch        epoch\n",
       "16     everything      everyth         everyth   everything\n",
       "17           evil         evil            evil         evil\n",
       "18           face         face             fac         face\n",
       "19           fair         fair            fair         fair\n",
       "20            far          far             far          far\n",
       "21    foolishness      foolish            fool  foolishness\n",
       "22            for          for             for          for\n",
       "23         france        franc           frant       france\n",
       "24          going           go           going        going\n",
       "25           good         good            good         good\n",
       "26            had          had             had          had\n",
       "27         heaven       heaven            heav       heaven\n",
       "28           hope         hope             hop         hope\n",
       "29             in           in              in           in\n",
       "30    incredulity     incredul          incred  incredulity\n",
       "31       insisted       insist          insist     insisted\n",
       "32             it           it              it           it\n",
       "33            its           it              it           it\n",
       "34            jaw          jaw             jaw          jaw\n",
       "35           king         king            king         king\n",
       "36          large         larg            larg        large\n",
       "37          light        light           light        light\n",
       "38           like         like             lik         like\n",
       "39       noisiest     noisiest        noisiest     noisiest\n",
       "40        nothing         noth            noth      nothing\n",
       "41             of           of              of           of\n",
       "42             on           on              on           on\n",
       "43           only         onli              on         only\n",
       "44             or           or              or           or\n",
       "45          other        other             oth        other\n",
       "46         period       period          period       period\n",
       "47          plain        plain           plain        plain\n",
       "48        present      present            pres      present\n",
       "49          queen        queen           queen        queen\n",
       "50       received       receiv          receiv     received\n",
       "51         season       season          season       season\n",
       "52          short        short           short        short\n",
       "53             so           so              so           so\n",
       "54           some         some             som         some\n",
       "55         spring       spring          spring       spring\n",
       "56    superlative       superl          superl  superlative\n",
       "57           that         that            that         that\n",
       "58            the          the             the          the\n",
       "59          there        there            ther        there\n",
       "60         throne       throne          throne       throne\n",
       "61          times         time             tim         time\n",
       "62             to           to              to           to\n",
       "63             us           us              us            u\n",
       "64            was           wa             was           wa\n",
       "65            way          way             way          way\n",
       "66             we           we              we           we\n",
       "67           were         were             wer         were\n",
       "68         winter       winter            wint       winter\n",
       "69         wisdom       wisdom          wisdom       wisdom\n",
       "70           with         with            with         with\n",
       "71          worst        worst           worst        worst"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sorted(set([token.lower() for token in two_sentences_tokens if token.isalpha()]))\n",
    "\n",
    "tokens_ps = ps_stemmer(tokens)\n",
    "tokens_ls = ls_stemmer(tokens)\n",
    "wnl = WordNetLemmatizer()\n",
    "tokens_lemmas = [wnl.lemmatize(t) for t in tokens]\n",
    "compare = zip(tokens, tokens_ps, tokens_ls, tokens_lemmas)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.DataFrame(compare, columns=[\"Original Token\", \"Porter Stems\", \"Lancaster Stems\", \"Lemmas\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Porter valid roots (59/71): a, age, all, and, be, belief, best, comparison, dark, despair, direct, england, epoch, evil, face, fair, far, foolish, for, go, good, had, heaven, hope, in, insist, it, it, jaw, king, light, like, of, on, or, other, period, plain, present, queen, season, short, so, some, spring, that, the, there, throne, time, to, us, way, we, were, winter, wisdom, with, worst\n",
    "* Lancaster valid roots (43/71): a, and, best, comparison, dark, despair, direct, england, epoch, evil, fair, far, fool, for, good, had, in, insist, it, it, jaw, king, light, of, on, or, period, plain, queen, season, short, so, spring, that, the, throne, to, us, way, we, wisdom, with, worst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
