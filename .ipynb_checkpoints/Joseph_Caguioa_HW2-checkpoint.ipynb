{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joseph Caguioa\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "DS 7337: Natural Language Processing\n",
    "\n",
    "Section 404 (Tuesday 2030-2200)\n",
    "\n",
    "HW2 Due: Date of Live Session 4 (1/28/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u><a name=\"toc\">Table of Contents:</a></u>\n",
    "* [Question 1](#question1)\n",
    "* [Question 2](#question2)\n",
    "* [Question 3](#question3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk import *\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question1\">Question 1</a> \n",
    "\n",
    "<b>In Python, create a method for scoring the vocabulary size of a text, and normalize the score from 0 to 1. It does not matter what method you use for normalization as long as you explain it in a short paragraph. (Various methods will be discussed in the live session.)</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bird-Klein-Loper text contains a code snippet for calculating vocabulary size that involves counting the set of all words (accounting for case), not including numbers and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16948"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word.lower() for word in text1 if word.isalpha())) # Should return 16948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this differs from the lexical diversity score defined earlier in the chapter. That function gets unique tokens (types), which is stated to indirectly indicate vocabulary size, but also includes punctuation and does not deduplicate based on case.\n",
    "\n",
    "One reasonable method for normalizing the vocabulary score would be to divide the vocabulary size by the total number of words in the text. That is to say, strip away all tokens with non-alphabetic characters to obtain all words, and then find the percentage of unique words within that total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def vocabulary(text):\n",
    "    '''\n",
    "    Function to obtain unique vocabulary from text.\n",
    "    Adapted definition from pg25 of Bird-Klein-Loper's NLP with Python.\n",
    "    \n",
    "    Parameters: \n",
    "        text (list): Text as list of tokens\n",
    "    \n",
    "    Returns:\n",
    "        list: Set of unique words\n",
    "    '''\n",
    "    \n",
    "    vocab = set(word.lower() for word in text if word.isalpha())\n",
    "    return vocab\n",
    "\n",
    "def words_only(text):\n",
    "    '''\n",
    "    Function to remove non-alphabetic tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        text (list): Text as list of tokens\n",
    "    \n",
    "    Returns:\n",
    "        list: List of all words in text, including duplicates\n",
    "    '''\n",
    "    \n",
    "    words = list(word for word in text if word.isalpha())\n",
    "    return words\n",
    "\n",
    "def vocab_diversity(text):\n",
    "    '''\n",
    "    Function to get percent of unique vocabulary\n",
    "    \n",
    "    Parameters:\n",
    "        text (list): Text as list of tokens\n",
    "    \n",
    "    Returns:\n",
    "        float: Percentage of unique vocabulary in word total\n",
    "    '''\n",
    "    \n",
    "    return len(vocabulary(text))/len(words_only(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size of example 1 is 0.5.\n",
      "The vocabulary size of example 2 is 0.6666666666666666.\n"
     ]
    }
   ],
   "source": [
    "example_sentence_1 = ['A', 'a', 'An', 'an', 'The', 'the', ';', '.', 'ABC123']\n",
    "example_sentence_2 = ['A', 'a', 'antidisestablishmentarianism', '.']\n",
    "\n",
    "print(f\"The vocabulary size of example 1 is {vocab_diversity(example_sentence_1)}.\")\n",
    "print(f\"The vocabulary size of example 2 is {vocab_diversity(example_sentence_2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question2\">Question 2</a> \n",
    "\n",
    "<b>After consulting section 3.2 in chapter 1 of Bird-Klein, create a method for scoring the long-word vocabulary size of a text, and likewise normalize (and explain) the scoring as in step 1 above.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bird-Klein-Loper 3.2 identifies long words as those with more than 15 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CIRCUMNAVIGATION', 'uncomfortableness', 'cannibalistically', 'circumnavigations', 'superstitiousness', 'apprehensiveness', 'indiscriminately', 'indiscriminately', 'superstitiousness', 'comprehensiveness', 'circumnavigating', 'preternaturalness', 'circumnavigation', 'apprehensiveness', 'indiscriminately', 'simultaneousness', 'indispensableness', 'apprehensiveness', 'undiscriminating', 'irresistibleness', 'Physiognomically', 'physiognomically', 'physiognomically', 'circumnavigation', 'hermaphroditical', 'circumnavigating', 'characteristically', 'comprehensiveness', 'comprehensiveness', 'uncompromisedness', 'uninterpenetratingly', 'responsibilities', 'supernaturalness', 'subterraneousness', 'apprehensiveness', 'simultaneousness']\n"
     ]
    }
   ],
   "source": [
    "text1_long_words = [word for word in text1 if len(word) > 15]\n",
    "print(text1_long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the example list above contains some repeated words after taking capitalization and plurality into account (e.g., circumnavigation, comprehensiveness). In order to score long-word vocabulary size, it would make sense to consider the percentage of the set of unique long words (that is, ignoring case) out of the total vocabulary. Doing this gives a better sense for vocabulary complexity than using the word total or token total as the normalizing quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_vocabulary(text):\n",
    "    '''\n",
    "    Function to get set of \"long\" words.\n",
    "    \n",
    "    Parameters:\n",
    "        text (list): Text as list of tokens\n",
    "    \n",
    "    Returns:\n",
    "        list: Set of unique words with more than 15 characters\n",
    "    '''\n",
    "    \n",
    "    vocab = vocabulary(text)\n",
    "    long_vocabulary = [word for word in vocab if len(word) > 15]\n",
    "    return long_vocabulary\n",
    "    \n",
    "def long_vocabulary_size(text):\n",
    "    '''\n",
    "    Function to \n",
    "    \n",
    "    Parameters:\n",
    "        text (list): Text as list of tokens\n",
    "        \n",
    "    Returns:\n",
    "        float: Percentage of long words out of set of vocabulary\n",
    "    '''\n",
    "\n",
    "    return len(long_vocabulary(text))/len(vocabulary(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The long-word vocabulary size of example 1 is 0.0.\n",
      "The long-word vocabulary size of example 2 is 0.5.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The long-word vocabulary size of example 1 is {long_vocabulary_size(example_sentence_1)}.\")\n",
    "print(f\"The long-word vocabulary size of example 2 is {long_vocabulary_size(example_sentence_2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"question3\">Question 3</a> \n",
    "\n",
    "<b>Now create a “text difficulty score” by combining the lexical diversity score from homework 1, and your normalized score of vocabulary size and long-word vocabulary size, in equal weighting. Explain what you see when this score is applied to the same graded texts you used in homework 1.</b> <sub>[(back to top)](#toc)</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06230453042623537\n"
     ]
    }
   ],
   "source": [
    "# Lexical diversity. Definition from pg9 of Bird/Klein/Loper's NLP with Python\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "print(lexical_diversity(text3)) # Should return 0.06230453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import package needed to access texts as urls\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "def tokenize_url(url):\n",
    "    \"\"\"\n",
    "    Function to obtain text from a web address and return list of tokens.\n",
    "    \n",
    "    Parameters:\n",
    "        url (string): Url of interest\n",
    "    \n",
    "    Returns:\n",
    "        list: Text from url as list of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    response = http.request('GET', url)\n",
    "    raw = response.data.decode('utf-8')\n",
    "    return word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_url = 'http://www.gutenberg.org/cache/epub/14880/pg14880.txt'\n",
    "fourth_tokens = tokenize_url(fourth_url)\n",
    "\n",
    "fifth_url = 'http://www.gutenberg.org/cache/epub/15040/pg15040.txt'\n",
    "fifth_tokens = tokenize_url(fifth_url)\n",
    "\n",
    "sixth_url = 'http://www.gutenberg.org/cache/epub/16751/pg16751.txt'\n",
    "sixth_tokens = tokenize_url(sixth_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above contents to get the texts are taken from my Homework 1 notebook. Equally weighting the three scores could simply translate as taking their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_difficulty_score(text):\n",
    "    '''\n",
    "    Function to calculate text difficulty using equal weights of lexical diversity, \n",
    "    vocabulary size, and long-word vocabulary size.\n",
    "    \n",
    "    Parameters:\n",
    "        text (list): Text as list of tokens \n",
    "    \n",
    "    Returns:\n",
    "        float: Average of three text difficulty parameters\n",
    "    '''\n",
    "    \n",
    "    return (lexical_diversity(text) + vocab_diversity(text) + long_vocabulary_size(text))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text difficulty of McGuffey's Fourth Eclectic Reader is 0.08133.\n",
      "The text difficulty of Mcguffey's Fifth Eclectic Reader is 0.07462.\n",
      "The text difficulty of McGuffey's Sixth Eclectic Reader is 0.06678.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The text difficulty of McGuffey's Fourth Eclectic Reader is \\\n",
    "{round(text_difficulty_score(fourth_tokens),5)}.\")\n",
    "print(f\"The text difficulty of Mcguffey's Fifth Eclectic Reader is \\\n",
    "{round(text_difficulty_score(fifth_tokens),5)}.\")\n",
    "print(f\"The text difficulty of McGuffey's Sixth Eclectic Reader is \\\n",
    "{round(text_difficulty_score(sixth_tokens),5)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, perhaps unexpectedly, this created \"text difficulty score\" decreases as the reading level increases. Why might this be? Some investigation can be done using len() on the vocabulary(), words_only(), and long_vocabulary() functions defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score | Fourth Reader | Fifth Reader | Sixth Reader\n",
    "-|-|-|-\n",
    "Word Total | 63404 | 97807 | 138387\n",
    "Vocabulary | 7629 | 10836 | 13713\n",
    "Vocab Diversity | 0.12032 | 0.11079 | 0.099092\n",
    "Long Words | 1 | 2 | 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the nature of these introduction to reading textbooks, the long-word vocabulary size has little bearing; each book contains no more than a handful of big words, which are drops in the bucket compared to general vocabulary size. Of more interest is vocabulary size, which still appears outstripped by the general word counts. This is expected, as the structure of readers will typically have repetition for the words they introduce.\n",
    "\n",
    "Perhaps like with lexical diversity, this attempt at normalization still does not do well at accounting for text size. Maybe methods that look at difficulty measures beyond word counts, such as sentence structure, can give improved insight."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
